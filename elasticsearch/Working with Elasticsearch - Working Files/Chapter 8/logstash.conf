# adapted from http://blog.sematext.com/2015/01/19/grok-elasticsearch-logs-with-logstash/

input {
  file { 
    path => "/opt/elasticsearch-2.0.0/logs/*.log"  # tail ES log and slowlogs
    type => "elasticsearch"
    start_position => "beginning"  # parse existing logs, too
    codec => multiline {   # put the whole exception in a single event
      pattern => "^\["
      negate => true
      what => "previous"
    }
  }
}
 
filter {
  if [type] == "elasticsearch" {
    grok {  # parses the common bits
      match => [ "message", "\[%{TIMESTAMP_ISO8601:timestamp}\]\[%{DATA:severity}%{SPACE}\]\[%{DATA:source}%{SPACE}\]%{SPACE}(?<message>(.|\r|\n)*)" ]
      overwrite => [ "message" ]
    }
 
    if [source] == "org.elasticsearch.index.search.slowlog.fetch" or [source] == "org.elasticsearch.index.search.slowlog.query" {
      grok {
        match => [ "message", "took\[%{DATA:took}\], took_millis\[%{DATA:took_millis}\], types\[%{DATA:types}\], stats\[%{DATA:stats}\], search_type\[%{DATA:search_type}\], total_shards\[%{DATA:total_shards}\], source\[(?<source_body>(.|\r|\n)*\], extra_source\[)%{DATA:extra_source}" ]
      }
      mutate {
        remove_field => [ "message" ]
        gsub => [ "source_body", "\], extra_source\[$", "" ]
      }
    }
 
    date { # use timestamp from the log
      "match" => [ "timestamp", "YYYY-MM-dd HH:mm:ss,SSS" ]
      target => "@timestamp"
    }
     
    mutate {
      remove_field => [ "timestamp" ]  # remove unused stuff
      convert => {  # type numeric fields (they're strings by default)
        "took_millis" => "integer"
        "total_shards" => "integer"
        "shard" => "integer"
      }
    }
  }
}
 
output {
  elasticsearch {   # send everything to Logsene
    host => "logsene-receiver.sematext.com"
    ssl => true  # works with Logstash 1.5+
    port => 443  # use 80 for plain HTTP
    index => "LOGSENE_TOKEN"  # fill in your token (click Integration from your Logsene app)
    protocol => "http"
    manage_template => false
  }
}
